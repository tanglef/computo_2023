---
title: "Peerannot: learning from crowdsourced image datasets with Python"
subtitle: ""
author:
  - name: Tanguy Lefort
    corresponding: true
    email: tanguy.lefort@umontpellier.fr
    url: https://tanglef.github.io
    orcid: 0009-0000-6710-3221
    affiliations:
      - name: Name of Affiliation one
        department: Statistics
        url: https://someplace.themoon.org
  - name: Benjamin Charlier
    email: benjamin.charlier@umontpellier.fr
    url: https://imag.umontpellier.fr/~charlier/index.php?page=index&lang=en
    affiliations:
      - name: Name of Afficiliation two
        department: Computer Science
        url: https://someplace.themoon.org
  - name: Alexis Joly
    email: alexis.joly@inria.fr
    url: http://www-sop.inria.fr/members/Alexis.Joly/wiki/pmwiki.php
    orcid: 0000-0002-2161-9940
    affiliations:
      - name: Name of Afficiliation two
        department: Computer Science
        url: https://someplace.themoon.org
  - name: Joseph Salmon
    email: joseph.salmon@umontpellier.fr
    url: http://josephsalmon.eu/
    orcid: 0000-0002-3181-0634
    affiliations:
      - name: Name of Afficiliation two
        department: Computer Science
        url: https://someplace.themoon.org
date: last-modified
date-modified: last-modified
description: |
  Crowdsourcing is a fast and easy way to collect labels for large datasets.
  However, it is common for workers to disagree with each other.
  The sources of errors might come from the workers' abilities, but also from the task's intrinsic difficulty.
  We introduce peerannot: a Python library to handle crowdsourced labels and learn from them.
abstract: >+
  Crowdsourcing is a fast and easy way to collect labels for large datasets.
  However, it is common for workers to disagree with each other.
  The sources of errors might come from the workers' abilities, but also from the task's intrinsic difficulty.
  We introduce `peerannot`: a `Python` library to handle crowdsourced labels and learn from them.
  Our library allows users to aggregate labels from common noise models or train a deep-learning based classifier directly from crowdsourced labels.
  Moreover, we provide an identification module to explore both the datasets' tasks difficulty and workers abilities easily.
keywords: [crowdsourcing, label noise, task difficulty, worker ability]
citation:
  type: article-journal
  container-title: "Computo"
  doi: "xxxx"
  url: https://computo.sfds.asso.fr/template-computo-quarto
  publisher: "Société Française de Statistique"
  issn: "2824-7795"
bibliography: references.bib
github-user: computorg
repo: "template-computo-python"
draft: true # set to false once the build is running
published: false # will be set to true once accepted
format:
  computo-html: default
  computo-pdf: default
jupyter: python3
execute:  # to remove at the end
  cache: true
---

# TODOS:

  - interpreation of results !
  - Deep learning part / ECE issue with small datasets (XXX neurips NOLA)
  - images / schemes
  - XXXs
  - DRAWINGS and colors!
  - identify module should be split into worker / tasks
  - Tr(pi) for CL / CoNAL should only be run if CoNAL agg-deep is done

# Introduction: crowdsourcing in image classification

Image datasets widely use crowdsourcing to collect labels.
This allows workers to annotate images for a small cost and faster than using expert labeling.
Many classical datasets using machine learning have been created with human intervention to create labels, such as CIFAR-$10$, [@krizhevsky2009learning],
ImageNet [@imagenet_cvpr09] or [@Garcin_Joly_Bonnet_Affouard_Lombardo_Chouet_Servajean_Lorieul_Salmon2021] in image classification, but also COCO [@cocodataset], solar photovoltaic arrays [@kasmi2023crowdsourced] or even macro litter [@chagneux2023] in image segmentation and object counting.

Crowdsourced datasets induce at least three entangled problems to which we contribute with `peerannot`:

  - *How to aggregate multiple labels into a single label from crowdsourced tasks?* This problem occurs for example when dealing with a single dataset that has been labeled by multiple workers where there are disagreements. But it is also encountered with polls, reviewing, *etc.*
  - *How to learn a classifier from crowdsourced datasets?* Where the first question is bound by aggregating multiple labels into a single one, this considers the case where we don't need a label to train on, but we need to train a classifier with the crowdsourced data that performs well on a testing set. This is the most common case in machine learning, however, this means that we need the actual tasks to train on -- and in crowdsourced datasets, they are not always available.
  - *How to score workers in a crowdsourcing experiment?* Beyond learning a classifier or inferring the true label, one could wish to find a ranking between workers. For example, it is relevant for the gamification of labeling. **XXX**

The library `peerannot` addresses these practical questions within a reproducible setting. Indeed, the complexity of experiments often leads to a lack of transparency and reproducible results for simulations and real datasets.
We propose standard simulation settings with explicit parameter implementations that can be shared.
For real datasets, `peerannot` is compatible with standard neural networks architectures from the `Torchvision` [@torchvision] library and `Pytorch` [@pytorch], allowing a flexible framework with easy-to-share scripts to reproduce experiments.

# Notation and package structure

## Crowdsourcing notation

Let us consider the classical supervised learning classification framework. A training set $\mathcal{D}=\{(x_i, y_i^\star)\}_{i=1}^{n_{\text{task}}}$ is composed of $n_{\text{task}}$ tasks $x_i\in\mathcal{X}$ with ground truth label $y_i^\star \in\mathcal [K]={1,\dots,K}$ one of the $K$ possible classes.
In the following, the tasks considered are generally RGB images.

What differs from the classical setting with crowdsourced data, is that the ground truth $y_i^\star$ is unknown.
However, instead for a given task $x_i$, a worker $w_j$ proposes their label denoted $y_i^{(j)}$.
The set of workers answering the task $x_i$ is denoted $\mathcal{A}(x_i)=\{j: w_j \text{ answered }x_i\}$. The cardinal $\vert \mathcal{A}(x_i)\vert$ is called the feedback effort on the task $x_i$.
The feedback effort can not exceed the total number of workers $n_{\text{worker}}$.
Mirroring the point of view by considering the workers and not the tasks: the set of tasks answered by a worker $w_j$ is denoted $\mathcal{T}(w_j)=\{i: w_j \text{ answered } x_i\}$. The cardinal $\vert \mathcal{T}(w_j)\vert$ is called the workerload of $w_j$.

The final dataset is:
$$
\mathcal{D}_{\text{train}} := \bigcup_{i\in[n_\text{task}]} \{(x_i, (y_i^{(j)}) \text{ for }j\in\mathcal{A}(x_i))\} = \bigcup_{j\in[n_\text{worker}]} \{(x_i, (y_i^{(j)})) \text{ for }i \in\mathcal{T}(w_j)\} \enspace.
$$

In this article, we do not address the setting where workers report their self-confidence (XXX), nor settings where workers are presented a trapping set -- *i.e* a subset of tasks where the ground truth is known to evaluate them with known labels [@khattak_toward_2017].

## Storing crowdsourced datasets in `peerannot`

To store crowdsourcing datasets efficiently and in a standardized fashion, `peerannot` proposes the following structure:

```{bash}
datasetname
      ├── train
      │     ├── class0
      │     │     ├─ task0-<vote_index_0>.png
      │     │     ├─ task1-<vote_index_1>.png
      │     │     ├─ ...
      │     │     └─ taskn0-<vote_index_n0>.png
      │     ├── class1
      │     ├── ...
      │     └── classK
      ├── val
      ├── test
      ├── dataset.py (optional)
      ├── metadata.json
      └── answers.json
```

Each dataset is its folder.
If the tasks (images) are available, they must be stored as it is usual to store `ImageFolder` datasets with `pytorch` into a `train`, `val` and `test` folder.
Each image can have its name followed by its index in the `answers.json` file.

The `answers.json` file contains the different votes for each task as follows:

```json
{
    0: {1: 2, 2: 2},
    1: {0: 1, 1:1, 2:0}
}
```

In this example, there are two tasks, $n_{worker}=3$ workers and $K=3$ classes.
For the task $1$, the feedback effort is $\vert\mathcal{A}(x_1)\vert=3$ and the workerload of $w_2$ is $\vert\mathcal{T}(w_2)\vert=1$.

- talk about the metadata and the dataset.py file briefly ?

# Aggregation strategies in crowdsourcing

The first question we address with `peerannot` is: *How to aggregate multiple labels into a single label from crowdsourced tasks?*
The aggregation step can lead to two types of learnable labels $\hat y_i\in\Delta_{K}$ defined on the simplex of dimension $K-1$ depending on the use case:

- a **hard** label: $\hat y_i$ is a Dirac distribution, this can be encoded as a classical label in $[K]$,
- a **soft** label: $\hat y_i\in\Delta_{K}$ can be a probability distribution other than Dirac distribution, in that case, each coefficient in $\hat y_i$ represents the probability to belong to the given class.

Learning from soft labels has been shown to improve learning performance and make the classifier learn the task ambiguity. **XXX**
However, crowdsourcing is often used as a stepping stone to creating a new dataset and we usually expect a classification dataset to associate a task $x_i$ to a single label and not a full probability distribution.
In this case, we recommend in practice releasing the anonymous answered labels and the aggregation strategy used to reach a consensus on a single label.
With `peerannot`, both soft and hard labels can be produced.
Note that when a strategy produces a soft label, a hard label can be induced by taking the class with the maximum probability.

## Classical models

While the most intuitive way to create a label from multiple answers for any type of crowdsourced task would be to take the majority vote (MV), this strategy has many shortcomings [@james1998majority] -- there is no noise model, no worker reliability estimated, no task difficulty involved and especially no way to remove poorly performing workers. This baseline aggregation can be expressed as:

$$
\hat y_i^{\text{MV}} = \operatornamewithlimits{argmax}_{k\in[K]} \sum_{j\in\mathcal{A}(x_i)} 1_{\{y_i^{(j)}=k\}} \enspace.
$$

One pitfall with the MV is that the label produced is hard, hence the ambiguity is discarded by construction. To remedy this, the Naive Soft (NS) labeling consists in using the empirical frequency distribution as the task label:

$$
\hat y_i^{\text{NS}} = \bigg(\frac{1}{\vert\mathcal{A}(x_i)\vert}\sum_{j\in\mathcal{A}(x_i)} 1_{\{y_i^{(j)}=k\}} \bigg)_{j\in[K]} \enspace.
$$
With the NS label, we keep the ambiguity, but all workers and all tasks are put on the same level. In practice, it is known that each worker comes with their abilities, thus modeling this knowledge can produce better results.

Going further into the aggregation, researchers began creating a noise model to take into account the workers' abilities in the aggregation.
These types of models are most often EM-based and one of the most studied [@gao2013minimax] and applied [@servajean2017crowdsourcing;@rodrigues2018deep] is the Dawid and Skene's (DS) model [@dawid_maximum_1979].
Assuming the workers are answering tasks independently, this model boils down to model pairwise confusions between each possible class.
Each worker $w_j$ is assigned a confusion matrix $\pi^{(j)}\in\mathbb{R}^{K\times K}$ such that $\pi^{(j)}_{k\ell} = \mathbb{P}(y_i^{(j)}=\ell\vert y_i^\star=k)$.
The model assumes that the probability for a task $x_i$ to have true label $y_i^\star=k$ follows a multinomial distribution with probabilities $\pi^{(j)}_{k,\bullet}$ for each worker.
Each class has a prevalence $\rho_k=\mathbb{P}(y_i^\star=k)$ to appear in the dataset.
Using the independence between workers, we obtain the following likelihood to maximize (using the EM algorithm):

$$
\displaystyle\prod_{i\in [n_{\texttt{task}}]}\prod_{k \in [K]}\bigg[\rho_k\prod_{j\in [n_{\texttt{worker}}]}
    \prod_{k\in [K]}\big(\pi^{(j)}_{k, k}\big)^{1_{\{y_i^{(j)}=k\}}}
    \bigg]^{T_{ik}},
$$

with $T_{i,k}=1_{\{y_i^{\star}=k \}}$. The final aggregated soft label is $\hat y_i^{\text{DS}} = T_{i,\cdot}$.

Many variants of the DS model have been proposed in the literature, using Dirichlet priors on the confusion matrices [@passonneau-carpenter-2014-benefits], using $L$ clusters of workers [@imamura2018analysis] with $1\leq L\leq n_{\text{worker}}$ (DSWC) or even faster implementation that produces only hard labels [@sinha2018fast].

Finally, we present the GLAD model [@whitehill_whose_2009] that not only takes into account the worker's ability, but also the task difficulty in the noise model.
Denoting $\alpha_j\in\mathbb{R}^+$ the worker ability (the higher the better) and $\beta_i\in\mathbb{R}^+_\star$ the task's difficulty (the higher the easier), the model noise is:

$$
\mathbb{P}(y_i^{(j)}=y_i^\star\vert \alpha_j,\beta_i) = \frac{1}{1+\exp(-\alpha_j\beta_i)} \enspace.
$$
GLAD's model also assumes that the errors are uniform across wrong labels, thus:
$$
\forall k \in [K],\ \mathbb{P}(y_i^{(j)}=k\vert y_i^\star\neq k,\alpha_j,\beta_i) = \frac{1}{K-1}\left(1-\frac{1}{1+\exp(-\alpha_j\beta_i)}\right)\enspace.
$$
The likelihood can then be optimized using an EM algorithm to recover the soft label $\hat y_i^{\text{GLAD}}$.
All of these aggregation strategies -- and more -- are available in the `peerannot` library from the `peerannot.models` module.

## Experiments and evaluation of label aggregation strategies

One way to evaluate the label aggregation strategies is to measure their accuracy.
This means that the underlying ground truth must be known -- or at least for a representative subset.
As the set of $n_{\text{task}}$ can be seen as a training set for a future classifier, we denote this metric $\operatornamewithlimits{AccTrain}$ on a dataset $\mathcal{D}$ for a given aggregated label $(\hat y_i)_i$ as:

$$
\operatornamewithlimits{AccTrain}(\mathcal{D}) = \frac{1}{\vert \mathcal{D}\vert}\sum_{i=1}^{\vert\mathcal{D}\vert} 1_{\{y_i^\star=\operatornamewithlimits{argmax}_{k\in[K]}\hat y_i\}} \enspace.
$$

In the following, we write $\operatornamewithlimits{AccTrain}$ for $\operatornamewithlimits{AccTrain}(\mathcal{D}_{\text{train}})$ as we only consider the full training set so there is no ambiguity.
While this metric is useful, in practice there are a few arguable issues:

- the $\operatornamewithlimits{AccTrain}$ does not consider the ambiguity of the soft label, only the most probable class, whereas in some contexts ambiguity can be informative,
- in crowdsourcing one goal is to identify difficult tasks [@lefort2022improve], pruning those tasks can easily artificially improve the $\operatornamewithlimits{AccTrain}$, but there is no guarantee over the predictive performance of a model based on the newly pruned dataset.

We first consider classical simulation settings in the literature that can easily be created and reproduced using `peerannot`.

### Simulated independent mistakes

The independent mistakes consider that each worker $w_j$ answers following a multinomial distribution with weights given at the row $y_i^\star$ of their confusion matrix $\pi^{(j)}\in\mathbb{R}^{K\times K}$. Each confusion matrix is generated diagonally dominant.
Answers are independent of one another as each matrix is generated independently and each worker answers independently of other workers.
In this setting, the DS model is expected to perform the best with enough data as we are simulating data from its assumed noise model.

We simulate $n_{\text{task}}=200$ tasks and $n_{\text{worker}}=30$ workers with $K=5$ possible classes. Each task receives $\vert\mathcal{A}(x_i)\vert=10$ labels.

```{python}
#| output: false
! peerannot simulate --n-worker=30 --n-task=200  --n-classes=5 \
                     --strategy independent-confusion \
                     --feedback=10 --seed 0 \
                     --folder ./simus/independent
```


```{python}
#| code-fold: true
from peerannot.helpers.helpers_visu import feedback_effort, working_load
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from pathlib import Path
import matplotlib.ticker as mtick
sns.set_style("whitegrid")

plt.rcParams["text.usetex"] = True
plt.rcParams[
    "text.latex.preamble"
] = r"\usepackage{amsmath,amsfonts,amsthm,amssymb}"

votes_path = Path.cwd() / "simus" / "independent" / "answers.json"
metadata_path = Path.cwd() / "simus" / "independent" / "metadata.json"
efforts = feedback_effort(votes_path)
workerload = working_load(votes_path, metadata_path)
feedback = feedback_effort(votes_path)
nbins = 17
fig, ax = plt.subplots(1, 2, figsize=(9, 4))
sns.histplot(workerload, stat="percent", bins=nbins, shrink=1, ax=ax[0])
ax[0].yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))
ax[0].set_xlabel(r"$\vert\mathcal{T}(w_j)\vert$")
sns.histplot(feedback, stat="percent", bins=nbins, shrink=1, ax=ax[1])
ax[1].yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))
ax[1].set_xlabel(r"$\vert\mathcal{A}(x_i)\vert$")
for i in range(2):
  ax[i].xaxis.set_major_locator(MaxNLocator(integer=True))
  ax[i].xaxis.label.set_size(15)
  ax[i].yaxis.label.set_size(15)
  ax[i].xaxis.set_tick_params(labelsize=13)
  ax[i].yaxis.set_tick_params(labelsize=13)
  ax[i].title.set_size(18)
plt.tight_layout()
plt.show()
```

With the obtained answers, we can look at the aforementioned aggregation strategies performance:

```{python}
#| output: false
! for strat in MV NaiveSoft DS GLAD DSWC[L=5] DSWC[L=10]; do peerannot aggregate ./simus/independent/ -s $strat; done
```
```{python}
#| label: tbl-simu-independent
#| tbl-cap: AccTrain metric on simulated independent mistakes considering classical feature-blind label aggregation strategies
#| code-fold: true
import pandas as pd
import numpy as np
from IPython.display import display
simu_indep = Path.cwd() / 'simus' / "independent"
results = {"mv": [], "naivesoft": [], "glad": [], "ds": [], "dswc[l=5]": [], "dswc[l=10]": []}
for strategy in results.keys():
  path_labels = simu_indep / "labels" / f"labels_independent-confusion_{strategy}.npy"
  ground_truth = np.load(simu_indep / "ground_truth.npy")
  labels = np.load(path_labels)
  acc = (
          np.mean(labels == ground_truth)
          if labels.ndim == 1
          else np.mean(
              np.argmax(labels, axis=1)
              == ground_truth
          )
        )
  results[strategy].append(acc)
results = pd.DataFrame(results, index=['AccTrain'])
results.columns = map(str.upper, results.columns)
display(results)
```

### Simulated correlated mistakes

The correlated mistakes are also known as the student-teacher setting. Consider that the crowd of workers is divided into two categories: teachers and students such that $n_{\text{teacher}} + n_{\text{student}}=n_{\text{worker}}$. Each student is randomly assigned to one teacher at the beginning of the experiment. We generate the (diagonally dominant) confusion matrices of each teacher and the students are associated with their's teacher confusion matrix. Then, they all answer independently, following a multinomial distribution with weights given at the row $y_i^\star$ of their confusion matrix $\pi^{(j)}\in\mathbb{R}^{K\times K}$.

We simulate $n_{\text{task}}=200$ tasks and $n_{\text{worker}}=50$ with $80\%$ of students in the crowd. There are $K=5$ possible classes. Each task receives $\vert\mathcal{A}(x_i)\vert=10$ labels.

```{python}
#| output: false
! peerannot simulate --n-worker=50 --n-task=200  --n-classes=5 \
                     --strategy student-teacher \
                     --ratio 0.8 \
                     --feedback=10 --seed 0 \
                     --folder ./simus/student_teacher
```


```{python}
#| code-fold: true

votes_path = Path.cwd() / "simus" / "student_teacher" / "answers.json"
metadata_path = Path.cwd() / "simus" / "student_teacher" / "metadata.json"
efforts = feedback_effort(votes_path)
workerload = working_load(votes_path, metadata_path)
feedback = feedback_effort(votes_path)
nbins = 17
fig, ax = plt.subplots(1, 2, figsize=(9, 4))
sns.histplot(workerload, stat="percent", bins=nbins, shrink=1, ax=ax[0])
ax[0].yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))
ax[0].set_xlabel(r"$\vert\mathcal{T}(w_j)\vert$")
sns.histplot(feedback, stat="percent", bins=nbins, shrink=1, ax=ax[1])
ax[1].yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))
ax[1].set_xlabel(r"$\vert\mathcal{A}(x_i)\vert$")
for i in range(2):
  ax[i].xaxis.set_major_locator(MaxNLocator(integer=True))
  ax[i].xaxis.label.set_size(15)
  ax[i].yaxis.label.set_size(15)
  ax[i].xaxis.set_tick_params(labelsize=13)
  ax[i].yaxis.set_tick_params(labelsize=13)
  ax[i].title.set_size(18)
plt.tight_layout()
plt.show()
```

With the obtained answers, we can look at the aforementioned aggregation strategies performance:

```{python}
#| output: false
! for strat in MV NaiveSoft DS GLAD DSWC[L=5] DSWC[L=10]; do peerannot aggregate ./simus/student_teacher/ -s $strat; done
```
```{python}
#| label: tbl-simu-corr
#| tbl-cap: AccTrain metric on simulated correlated mistakes considering classical feature-blind label aggregation strategies
#| code-fold: true
simu_corr = Path.cwd() / 'simus' / "student_teacher"
results = {"mv": [], "naivesoft": [], "glad": [], "ds": [], "dswc[l=5]": [], "dswc[l=10]": []}
for strategy in results.keys():
  path_labels = simu_corr / "labels" / f"labels_student-teacher_{strategy}.npy"
  ground_truth = np.load(simu_corr / "ground_truth.npy")
  labels = np.load(path_labels)
  acc = (
          np.mean(labels == ground_truth)
          if labels.ndim == 1
          else np.mean(
              np.argmax(labels, axis=1)
              == ground_truth
          )
        )
  results[strategy].append(acc)
results = pd.DataFrame(results, index=['AccTrain'])
results.columns = map(str.upper, results.columns)
display(results)
```


### Simulated mistakes with discrete difficulty levels on tasks

For the final simulation setting, we consider the discrete difficulty presented in @whitehill_whose_2009. Contrary to other simulations, we here consider that each worker is either good or bad and each task is either easy or hard. Easy tasks are answered without mistakes by involved workers. However, hard tasks are answered following the worker's confusion matrix. The confusion matrix $\pi^{(j)}$ is diagonally dominant for good workers while each row is drawn uniformly in the simplex $\Delta_K$ for bad workers.
Each worker then answers independently to the presented tasks.

We simulate $n_{\text{task}}=500$ tasks and $n_{\text{worker}}=100$ with $30\%$ of good workers in the crowd and $50\%$ of easy tasks. There are $K=5$ possible classes. Each task receives $\vert\mathcal{A}(x_i)\vert=10$ labels.

```{python}
#| output: false
! peerannot simulate --n-worker=100 --n-task=200  --n-classes=5 \
                     --strategy discrete-difficulty \
                     --ratio 0.3 --ratio-diff 1 \
                     --feedback 10 --seed 0 \
                     --folder ./simus/discrete_difficulty
```


```{python}
#| code-fold: true

votes_path = Path.cwd() / "simus" / "discrete_difficulty" / "answers.json"
metadata_path = Path.cwd() / "simus" / "discrete_difficulty" / "metadata.json"
efforts = feedback_effort(votes_path)
workerload = working_load(votes_path, metadata_path)
feedback = feedback_effort(votes_path)
nbins = 17
fig, ax = plt.subplots(1, 2, figsize=(9, 4))
sns.histplot(workerload, stat="percent", bins=nbins, shrink=1, ax=ax[0])
ax[0].yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))
ax[0].set_xlabel(r"$\vert\mathcal{T}(w_j)\vert$")
sns.histplot(feedback, stat="percent", bins=nbins, shrink=1, ax=ax[1])
ax[1].yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))
ax[1].set_xlabel(r"$\vert\mathcal{A}(x_i)\vert$")
for i in range(2):
  ax[i].xaxis.set_major_locator(MaxNLocator(integer=True))
  ax[i].xaxis.label.set_size(15)
  ax[i].yaxis.label.set_size(15)
  ax[i].xaxis.set_tick_params(labelsize=13)
  ax[i].yaxis.set_tick_params(labelsize=13)
  ax[i].title.set_size(18)
plt.tight_layout()
plt.show()
```

With the obtained answers, we can look at the aforementioned aggregation strategies performance:

```{python}
#| output: false
! for strat in MV NaiveSoft DS GLAD DSWC[L=2] DSWC[L=5]; do peerannot aggregate ./simus/discrete_difficulty/ -s $strat; done
```
```{python}
#| label: tbl-simu-discrete-diff
#| tbl-cap: AccTrain metric on simulated mistakes when tasks are associated a difficulty level considering classical feature-blind label aggregation strategies
#| code-fold: true
simu_corr = Path.cwd() / 'simus' / "discrete_difficulty"
results = {"mv": [], "naivesoft": [], "glad": [], "ds": [], "dswc[l=2]": [], "dswc[l=5]": []}
for strategy in results.keys():
  path_labels = simu_corr / "labels" / f"labels_discrete-difficulty_{strategy}.npy"
  ground_truth = np.load(simu_corr / "ground_truth.npy")
  labels = np.load(path_labels)
  acc = (
          np.mean(labels == ground_truth)
          if labels.ndim == 1
          else np.mean(
              np.argmax(labels, axis=1)
              == ground_truth
          )
        )
  results[strategy].append(acc)
results = pd.DataFrame(results, index=['AccTrain'])
results.columns = map(str.upper, results.columns)
display(results)
```

# Learning from crowdsourced tasks

Most often, tasks are crowdsourced to create a large training set as modern machine learning models require more and more data.
The aggregation step then simply becomes the first step in the complete learning pipeline

PUT THE PIPELINE IN SCHEME HERE

## Classical models


- Introduce Crowdlayer
- CrowdLayer merged DS into the Deep learning world
- Tano introduced regularizations to help
- CoNAL too

## Prediction error when learning from crowdsourced tasks

The $\mathrm{AccTrain}$ metric presented in XXX might no longer be of interest when training a classifier. Classical error measurements involve a test dataset to estimate the generalization error.
To do so, we present hereafter two error metrics. Assuming we trained our classifier $f_\theta$ on a training set:
  - the test accuracy is computed as $\frac{1}{n_{\text{test}}}\sum_{i=1}^{n_{\text{test}}}1_{\{y_i^\star = \widehat{f_\theta(x_i)}\}}$
  - the expected calibration error [@guo_calibration_2017], computed as:
  XXX

Where the accuracy represents how well the classifier generalizes, the expected calibration error (ECE) quantifies the deviation between the accuracy and the confidence of the classifier. Modern neural networks are known to often be overconfident in their predictions XXX. However, it has also been remarked that training on crowdsourced data, depending on the strategy, mitigates this confidence issue. That is why we propose to compare them both in our coming experiments.
Note that the ECE error estimator is known to be biased [@gruber2022better].
Smaller training sets are known to have a higher ECE estimation error.
And in the crowdsourcing setting, openly available datasets are often quite small.

## Use case with `peerannot` on real datasets

Few real crowdsourcing experiments have been released publicly.
Among the available ones, CIFAR-10H [@peterson_human_2019] is one of the largest with $10 000$ tasks labeled by workers (the testing set of CIFAR-10).
The LabelMe dataset was extracted from crowdsourcing segmentation experimentation and a subset of $K=8$ classes was released in XXX.
Finally, a music genres classification with $K=10$ classes was crowdsourced.
The music dataset shows that with spectrography techniques, `peerannot` with image classification models can be used with audio files.

### LabelMe dataset

- on the importance of modeling common confusion

### Music dataset

- Another real dataset

# Exploring crowdsourced datasets

If a dataset requires citizen knowledge to be labeled, it is because expert knowledge is long and costly to obtain. In the era of big data, where datasets are built using web scraping XXX and other techniques, citizen science is popular as it is an easy way to produce many labels.

However, mistakes and confusions happen during these experiments.
Sometimes involuntarily (*e.g.* because the task is too hard or the worker is unable to differentiate between two classes) and sometimes not (*e.g.* the worker is a spammer).

Underlying all the learning models and aggregation strategies, the cornerstone of crowdsourcing is evaluating the trust we put in each worker depending on the presented task. And with the gamification of crowdsourcing XXX, it has become essential to find scoring metrics both for workers and tasks to keep citizens in the loop so to speak.
This is the purpose of the identification module in `peerannot`

## Exploring tasks' difficulty

To explore the tasks' intrinsic difficulty, we propose to compare three scoring metrics:

  - the entropy of the NS distribution: reliable with a big enough and not adversarial crowd, the entropy measures the inherent uncertainty of the distribution to the possible outcomes.
  - GLAD's scoring: by construction, @whitehill_whose_2009 introduced a scalar coefficient to score the difficulty of a task $\beta_i>0$.
  - the WAUM: introduced in XXX, this weighted area under the margins indicates how difficult it is for a model to classify the task given the crowdsourced labels and the trust we have in each worker.

Note that each of these statistics is useful in its context. The entropy can not be used in a setting with $|\mathcal{A}(x_i)|$ low (few labels per task), in particular for the LabelMe dataset it is not informative. The WAUM can work with any number of labels, but the larger the better. However, as it uses a deep learning classifier, the WAUM needs the tasks $(x_i)_i$ in addition to the proposed labels while the other strategies are feature-blind.


- entropy
- WAUM / GLAD use case
- introduce GLAD's limitations (1 label is impossible to work with because we don't see the actual tasks) -> do the 3 difficulties expe
- do an experiment with CIFAR-10H and 1<=|A(x_i)|<=5 labels with workers chosen randomly

## Exploring workers' reliability

Let us first consider simulated data with $n_{\text{worker}}=40$ workers. Each worker answers according to their confusion matrix.
Workers are dispatched as follows:
  KEEP GOING

```{python}
reliability = np.logspace(-1.5, 1, num=40)
rng = np.random.default_rng(0)
K = 5
matrices = []
arr = np.arange(K)
for j in range(len(reliability)):
  mat = rng.dirichlet([reliability[j]] * K, size=K)
  if reliability[j] <= 1:
    argmax_ = np.argmax(mat, axis=1)
    mat[arr, arr], mat[arr, argmax_] = mat[arr, argmax_], mat[arr, arr]
  matrices.append(mat)
matrices = np.stack(matrices)
np.save(Path.cwd() / "simus" / "confusions_exploring_reliability.npy", matrices)
```

```{python}
#| output: false
! peerannot simulate --n-worker=40 --n-task=200  --n-classes=5 \
                     --strategy independent-confusion \
                     --feedback=10 --seed 0 \
                     --folder ./simus/exploring_reliability \
                     --matrix-file ./simus/confusions_exploring_reliability.npy
```

From the labels, we can explore different worker evaluation scores.
GLAD's strategy estimates a reliability scalar coefficient $\alpha_j$ per worker.
With strategies looking to estimate confusion matrices, we investigate two scoring rules for workers:

- the trace of the confusion matrix: the closer to K the better the worker
- the spammer score [@raykar_ranking_2011] that is the Frobenius norm between the estimated confusion matrix $\hat\pi^{(j)}$ and the closest rank-$1$ matrix. The further to zero the better the worker.

Let us compare the ranking consistency between these three strategies on the simulated data.

..................

We can also compare workers' reliability in a real dataset.
When the tasks are available, confusion-matrix-based deep learning models can also be of use.
We thus add to the comparison the trace of the confusion matrices with CrowdLayer / CoNAL.

...................


# Conclusion

We introduced `peerannot`, a library to handle crowdsourced datasets. This library enables both easy label aggregation and direct training strategies with classical state-of-the-art classifiers. The identification module of the library allows exploring the collected data from both the tasks and the workers' point of view for better scorings and data cleaning procedures.
Our library also comes with templated datasets to better share crowdsourced datasets and have strategies more uniform to test on.

We hope that this library helps reproducibility in the crowdsourcing community and also standardizes training from crowdsourced datasets. New strategies can easily be incorporated into the open-source code available online. Finally, as `peerannot` is mostly directed to handle classification datasets, one of our future works would be to consider other `peerannot` modules to handle crowdsourcing for detection, segmentation and even worker evaluation in other contexts like peer-grading.
