---
title: "Peerannot: learn from crowdsourced datasets with Python"
subtitle: ""
author:
  - name: Tanguy Lefort
    corresponding: true
    email: tanguy.lefort@umontpellier.fr
    url: https://tanglef.github.io
    orcid: 0009-0000-6710-3221
    affiliations:
      - name: Name of Affiliation one
        department: Statistics
        url: https://someplace.themoon.org
  - name: Benjamin Charlier
    email: benjamin.charlier@umontpellier.fr
    url: https://imag.umontpellier.fr/~charlier/index.php?page=index&lang=en
    affiliations:
      - name: Name of Afficiliation two
        department: Computer Science
        url: https://someplace.themoon.org
  - name: Alexis Joly
    email: alexis.joly@inria.fr
    url: http://www-sop.inria.fr/members/Alexis.Joly/wiki/pmwiki.php
    orcid: 0000-0002-2161-9940
    affiliations:
      - name: Name of Afficiliation two
        department: Computer Science
        url: https://someplace.themoon.org
  - name: Joseph Salmon
    email: joseph.salmon@umontpellier.fr
    url: http://josephsalmon.eu/
    orcid: 0000-0002-3181-0634
    affiliations:
      - name: Name of Afficiliation two
        department: Computer Science
        url: https://someplace.themoon.org
date: last-modified
date-modified: last-modified
description: |
  Crowdsourcing is an fast and easy way to collect labels for large datasets.
  However, it is common for workers to disagree between each other.
  The sources of errors might come from the workers abilities, but also from the task's intrinsic difficulty.
  We introduce peerannot: a Python library to handle crowdsourced labels and learn from them.
abstract: >+
  Crowdsourcing is an fast and easy way to collect labels for large datasets.
  However, it is common for workers to disagree between each other.
  The sources of errors might come from the workers abilities, but also from the task's intrinsic difficulty.
  We introduce `peerannot`: a `Python` library to handle crowdsourced labels and learn from them.
  Our library allows users to: aggregate labels from common noise models or train a deep-learning classifier directly from crowdsourced labels.
  Moreover, we provide and identification module to explore the tasks given to label and their difficulty.
keywords: [crowdsourcing, label noise, task difficulty]
citation:
  type: article-journal
  container-title: "Computo"
  doi: "xxxx"
  url: https://computo.sfds.asso.fr/template-computo-quarto
  publisher: "Société Française de Statistique"
  issn: "2824-7795"
bibliography: references.bib
github-user: computorg
repo: "template-computo-python"
draft: true # set to false once the build is running
published: false # will be set to true once accepted
format:
  computo-html: default
  computo-pdf: default
jupyter: python3
latex-tinytex: false
---

# Introduction: crowdsourcing in image classification

Image datasets widely use crowdsourcing to collect labels.
This allows workers to annotate images for a small cost and faster than using expert labeling.
Many classical datasets using machine learning have been created with human intervention to create labels, as CIFAR-$10$, [@krizhevsky2009learning],
ImageNet [@imagenet_cvpr09] or [@Garcin_Joly_Bonnet_Affouard_Lombardo_Chouet_Servajean_Lorieul_Salmon2021] in image classification, but also COCO [@cocodataset], solar photovoltaic arrays [@kasmi2023crowdsourced] or even macro litter [@chagneux2023] in image segmentation and object counting.

Crowdsourced datasets induce at least three entangled problems to which we contribute with `peerannot`:

  - *How to aggregate multiple labels into a single label from crowdsourced tasks?* This problem occurs for example when dealing with a single dataset that has been labeled by multiple workers where there are disagreements. But it is also encountered with polls, reviewing, *etc.*
  - *How to learn a classifier from crowdsourced datasets?* Where the first question is bound by aggregating multiple labels into a single one, this considers the case where we don't need a label to train on, but we need to train a classifier with the crowdsourced data that performs well on a testing set. This is the most common case in machine learning, however, this means that we need the actual tasks to train on -- and in crowdsourced datasets, they are not always available.
  - *How to score workers in a crowdsourcing experiment?* Beyond learning a classifier or infer the true label, one could wish to find a ranking between workers. For example, it is relevant for the gamification of labeling. **REF**

The library `peerannot` addresses these practical questions within a reproducible setting. Indeed, the complexity of experiments often leads to a lack of transparency and reproducible results for simulations and real datasets.
We propose standard simulation settings with explicit parameter implementations that can be shared.
For real datasets, `peerannot` is compatible with standard neural networks architectures from the `Torchvision` [@torchvision] library and `Pytorch` [@pytorch], allowing a flexible framework with easy-to-share scripts to reproduce experiments.

# Notation and package structure

## Crowdsourcing notation

Let us consider the classical supervised learning classification framework. A training set $\mathcal{D}=\{(x_i, y_i^\star)\}_{i=1}^{n_{\text{task}}}$ is composed of $n_{\text{task}}$ tasks $x_i\in\mathcal{X}$ with ground truth label $y_i^\star \in\mathcal [K]={1,\dots,K}$ one of the $K$ possible classes.
In the following, the tasks considered are generally RGB images.

What differs from the classical setting with crowdsourced data, is that the ground truth $y_i^\star$ is unknown.
However, instead for a given task $x_i$, a worker $w_j$ proposes their label denoted $y_i^{(j)}$.
The set of workers answering the task $x_i$ is denoted $\mathcal{A}(x_i)=\{j: w_j \text{ answered }x_i\}$. The cardinal $\vert \mathcal{A}(x_i)\vert$ is called the feedback effort on the task $x_i$.
The feedback effort can not exceed the total number of workers $n_{\text{worker}}$.
Mirroring the point of view by considering the workers and not the tasks: the set of tasks answered by a worker $w_j$ is denoted $\mathcal{T}(w_j)=\{i: w_j \text{ answered } x_i\}$. The cardinal $\vert \mathcal{T}(w_j)\vert$ is called the workerload of $w_j$.

The final dataset is:
$$
\mathcal{D}_{\text{train}} := \bigcup_{i\in[n_\text{task}]} \{(x_i, (y_i^{(j)}) \text{ for }j\in\mathcal{A}(x_i))\} = \bigcup_{j\in[n_\text{worker}]} \{(x_i, (y_i^{(j)})) \text{ for }i \in\mathcal{T}(w_j)\} \enspace.
$$

In this article, we do not address the setting where workers report their self-confidence (REF), nor settings where workers are presented a trapping set -- *i.e* a subset of tasks where the ground truth is known to evaluate them with known labels [@khattak_toward_2017].

## Storing crowdsourced datasets in `peerannot`

To store crowdsourcing datasets efficiently and in a standardized fashion, `peerannot` proposes the following structure:

```{bash}
datasetname
      ├── train
      │     ├── class0
      │     │     ├─ task0-<vote_index_0>.png
      │     │     ├─ task1-<vote_index_1>.png
      │     │     ├─ ...
      │     │     └─ taskn0-<vote_index_n0>.png
      │     ├── class1
      │     ├── ...
      │     └── classK
      ├── val
      ├── test
      ├── dataset.py (optional)
      ├── metadata.json
      └── answers.json
```

Each dataset is its folder.
If the tasks (images) are available, they must be stored as it is usual to store `ImageDataset`s with `pytorch` into a `train`, `val` and `test` folder.
Each image can have its name followed by its index in the `answers.json` file.

The `answers.json` file contains the different votes for each task as follows:

```json
{
    0: {1: 2, 2: 2},
    1: {0: 1, 1:1, 2:0}
}
```

In this example, there are two tasks, $n_{worker}=3$ workers and $K=3$ classes.
For the task $1$, the feedback effort is $\vert\mathcal{A}(x_1)\vert=3$ and the workerload of $w_2$ is $\vert\mathcal{T}(w_2)\vert=1$.

- talk about the metadata and the dataset.py file briefly ?

# Aggregation strategies in crowdsourcing

The first question we address with `peerannot` is: *How to aggregate multiple labels into a single label from crowdsourced tasks?*
The aggregation step can lead to two types of learnable labels $\hat y_i\in\Delta_{K}$ defined on the simplex of dimension $K-1$ depending on the use case:

- a **hard** label: $\hat y_i$ is a Dirac distribution, this can be encoded as a classical label in $[K]$,
- a **soft** label: $\hat y_i\in\Delta_{K}$ can be a probability distribution other than Dirac distribution, in that case, each coefficient in $\hat y_i$ represents the probability to belong to the given class.

Learning from soft labels has been shown to improve learning performance and make the classifier learn the task ambiguity. **REF**
However, crowdsourcing is often used as a stepping stone to creating a new dataset and we usually expect a classification dataset to associate a task $x_i$ to a single label and not a full probability distribution.
In this case, we recommend in practice releasing the anonymous answered labels and the aggregation strategy used to reach a consensus on a single label.
With `peerannot`, both soft and hard labels can be produced.
Note that when a strategy produces a soft label, a hard label can be induced by taking the class with the maximum probability (and in case of equality the hard label is chosen for example with a coin toss).

## Classical models

While the most intuitive way to create a label from multiple answers for any type of crowdsourced task would be to take the majority vote (MV), this strategy has many shortcomings [@james1998majority] -- there is no noise model, no worker reliability estimated, no task difficulty involved and especially no way to remove poorly performing workers. This baseline aggregation can be expressed as:

$$
\hat y_i^{\text{MV}} = \operatornamewithlimits{argmax}_{k\in[K]} \sum_{j\in\mathcal{A}(x_i)} 1_{\{y_i^{(j)}=k\}} \enspace.
$$

One pitfall with the MV is that the label produced is hard label, hence the ambiguity is discarded by construction. To remedy this, the Naive Soft (NS) labeling consists in using the empirical frequency distribution as the task label:

$$
\hat y_i^{\text{MV}} = \bigg(\frac{1}{\vert\mathcal{A}(x_i)\vert}\sum_{j\in\mathcal{A}(x_i)} 1_{\{y_i^{(j)}=k\}} \bigg)_{j\in[K]} \enspace.
$$
With the NS label, we keep the ambiguity, but all workers and all tasks are put on the same level. In practice, it is known that each worker comes with their  abilities, thus modeling this knowledge can produce better results.

Going further into the aggregation, researchers began creating a noise model to take into account the workers' abilities in the aggregation.
These types of models are most often EM-based and one of the most studied [@gao2013minimax] and applied [@servajean2017crowdsourcing;@rodrigues2018deep] is the Dawid and Skene's (DS) model [@dawid_maximum_1979].
Assuming the workers are answering tasks independently, this model boils down to model pairwise confusions between each possible class.
Each worker $w_j$ is assigned a confusion matrix $\pi^{(j)}\in\mathbb{R}^{K\times K}$ such that $\pi^{(j)}_{k\ell} = \mathbb{P}(y_i^{(j)}=\ell\vert y_i^\star=k)$.
The model assumes that the probability for a task $x_i$ to have true label $y_i^\star=k$ follows a multinomial distribution with probabilities $\pi^{(j)}_{k,\bullet}$ for each worker.
Each class has a prevalence $\rho_k=\mathbb{P}(y_i^\star=k)$ to appear in the dataset.
Using the independence between workers, we obtain the following likelihood to maximize (using the EM algorithm):

$$
\displaystyle\prod_{i\in [n_{\texttt{task}}]}\prod_{k \in [K]}\bigg[\rho_k\prod_{j\in [n_{\texttt{worker}}]}
    \prod_{k\in [K]}\big(\pi^{(j)}_{k, k}\big)^{1_{\{y_i^{(j)}=k\}}}
    \bigg]^{T_{ik}},
$$

with $T_{i,k}=1_{\{y_i^{\star}=k \}}$. The final aggregated soft label is $\hat y_i^{\text{DS}} = T_{i,\cdot}$.

Many variants of the DS model have been proposed in the literature, using Dirichlet priors on the confusion matrices [@passonneau-carpenter-2014-benefits], using clusters of workers [@imamura2018analysis] or even faster implementation that produces only hard labels [@sinha2018fast].

Finally, we present the GLAD model [@whitehill_whose_2009] that not only takes into account the worker's ability, but also the task difficulty in the noise model.
Denoting $\alpha_j\in\mathbb{R}^+$ the worker ability (the higher the better) and $\beta_i\in\mathbb{R}^+_\star$ the task's difficulty (the higher the easier), the model noise is:

$$
\mathbb{P}(y_i^{(j)}=y_i^\star\vert \alpha_j,\beta_i) = \frac{1}{1+\exp(-\alpha_j\beta_i)} \enspace.
$$
GLAD's model also assumes that the errors are uniform across wrong labels, thus:
$$
\forall k \in [K],\ \mathbb{P}(y_i^{(j)}=k\vert y_i^\star\neq k,\alpha_j,\beta_i) = \frac{1}{K-1}\left(1-\frac{1}{1+\exp(-\alpha_j\beta_i)}\right)\enspace.
$$
The likelihood can then be optimized using an EM algorithm to recover the soft label $\hat y_i^{\text{GLAD}}$.
All of these aggregation strategies -- and more -- are available in the `peerannot` library from the `peerannot.models` module.

## Experiments and evaluation of label aggregation strategies

One way to evaluate the label aggregation strategies is to measure their accuracy.
This means that the underlying ground truth must be known -- or at least for a representative subset.
As the set of $n_{\text{task}}$ can be seen as a training set for a future classifier, we denote this metric $\operatornamewithlimits{AccTrain}$ on a dataset $\mathcal{D}$ for a given aggregated label $(\hat y_i)_i$ as:

$$
\operatornamewithlimits{AccTrain}(\mathcal{D}) = \frac{1}{\vert \mathcal{D}\vert}\sum_{i=1}^{\vert\mathcal{D}\vert} 1_{\{y_i^\star=\operatornamewithlimits{argmax}_{k\in[K]}\hat y_i\}} \enspace.
$$

In the following, we write $\operatornamewithlimits{AccTrain}$ for $\operatornamewithlimits{AccTrain}(\mathcal{D}_{\text{train}})$ as we only consider the full training set so there is no ambiguity.
While this metric is useful, in practice there are a few arguable issues:

- the $\operatornamewithlimits{AccTrain}$ does not consider the ambiguity of the soft label, only the most probable class, whereas in some contexts ambiguity can be informative,
- in crowdsourcing one goal is to identify difficult tasks [@lefort2022improve], pruning those tasks can easily artificially improve the $\operatornamewithlimits{AccTrain}$, but there is no guarantee over the predictive performance of a model based on the newly pruned dataset.

- introduce Acctrain
- Explain why Acctrain might not be reliable

### Simulated independent mistakes

### Simulated correlated mistakes

### Simulated mistakes with discrete difficulty levels on tasks

### Real datasets
- 1/2 experiments with classical real datasets

# Learning from crowdsourced tasks

## Prediction error when learning from crowdsourced tasks

- Classical errors can be measured on test sets: Pred accuracy, ECE...

### Classical deep learning-based strategies

- Introduce Crowdlayer
- CrowdLayer merged DS into the Deep learning world
- Tano introduced regularizations to help
- CoNAL too

### Use case with `peerannot` on real datasets

- examples in real datasets
- How to use hyperparameters for TanoReg or CoNAL

#### LabelMe dataset

- on the importance of modeling common confusion

#### Music dataset

- Another real dataset

# Exploring crowdsourced datasets

- Identify module

## Exploring tasks' difficulty

- entropy
- WAUM / GLAD use case
- introduce GLAD's limitations (1 label is impossible to work with because we don't see the actual tasks) -> do the 3 difficulties expe
- do an experiment with CIFAR-10H and 1<=|A(x_i)|<=5 labels with workers chosen randomly

## Exploring workers' reliability

- GLAD
- Trace of confusion matrices (available in DS or DS-based models: including CrowdLayer)
- Spam score

# Discussion

# Conclusion